# Deep Learning

## Stochastic Gradient Descent
 
### Batch

A hyperparameter that defines the number of samples to work through before updating the internal model parameters.

- **Batch Gradient Descent**: Batch Size = Size of Training Set
- **Stochastic Gradient Descent**: Batch Size = 1
- **Mini-Batch Gradient Descent**: 1 < Batch Size < Size of Training Set

### Epoch

A hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset.

- **Learning curves**: 
It is common to create line plots that show epochs along the x-axis as time and the error or skill of the model on the y-axis. These plots are sometimes called learning curves. These plots can help to diagnose whether the model has over learned, under learned, or is suitably fit to the training dataset.


