# Optimization of Deep Learning

Loss functions in deep learning usually are non-convex. Thus, traditional optimization is not enough. We should modernize optimization for deep learning.

Traditional ML theory dictates $d$ must be smaller than $n$ to avoid overfitting. But, in practice $d>>n$ generalized extremely well.

**Convergence**: finding a solution that matches the training data

**Generalization**: good performance on unseen data

