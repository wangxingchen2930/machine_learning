# Attention Mechanisms

In neural networks, attention is a technique that mimics cognitive attention. The effect enhances some parts of the input data while diminishing other parts â€” the thought being that the network should devote more focus to that small but important part of the data. Learning which part of the data is more important than others depends on the context and is trained by gradient descent.

Attention mechanisms are firstly widely used in NLP

## Transformer

<img src="./transformer.svg" class="centerImage">